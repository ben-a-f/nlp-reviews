{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaf280e8-28e2-43de-b592-0988ce8c8d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (1.3.1)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/63/78/ed291d95116695b8b5d7469a931d7c2e83d942df0853915ee504cee98bcf/regex-2023.8.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached regex-2023.8.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk) (4.63.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk) (4.11.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (4.7.1)\n",
      "Using cached regex-2023.8.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (758 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.8.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc11aa78-a007-434c-adc2-52e81c9762ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "from google.cloud import storage\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96a070-9818-4a68-b9e4-6aac7542b583",
   "metadata": {},
   "source": [
    "First we import our data from a GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ace6534-ae65-419b-87bc-6749b9748098",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'bf-review-nlp'\n",
    "blob_name = 'raw_data/Digital_Music.json.gz'\n",
    "\n",
    "# Initialize GCS client and get the blob\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(bucket_name)\n",
    "blob = bucket.blob(blob_name)\n",
    "\n",
    "json_gz_bytes = blob.download_as_bytes()\n",
    "json_gz_file = io.BytesIO(json_gz_bytes)\n",
    "\n",
    "with gzip.open(json_gz_file, 'rb') as f:\n",
    "    df = pd.read_json(f, lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaa6e7a3-1132-4665-8ca1-9ca16af3a332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1584082 entries, 0 to 1584081\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count    Dtype \n",
      "---  ------      --------------    ----- \n",
      " 0   overall     1584082 non-null  int64 \n",
      " 1   verified    1584082 non-null  bool  \n",
      " 2   style       1310814 non-null  object\n",
      " 3   reviewText  1582629 non-null  object\n",
      "dtypes: bool(1), int64(1), object(2)\n",
      "memory usage: 37.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns for this project.\n",
    "df = df.drop([\"reviewTime\", \"reviewerID\", \"asin\", \"reviewerName\", \"summary\", \"unixReviewTime\", \"vote\", \"image\"], axis=1)\n",
    "\n",
    "# Check NAs and column types.\n",
    "df.info()\n",
    "\n",
    "# Drop rows with no review text.\n",
    "df = df.dropna(subset=[\"reviewText\"])\n",
    "\n",
    "# Check all scores are values 1-5. We convert to categorical because the scores aren't continuous.\n",
    "df[\"overall\"].unique()\n",
    "df[\"overall\"] = df['overall'].astype('category')\n",
    "\n",
    "# Check style values.\n",
    "# We clean up the formatting with regex. This also conveniently converts \"nan\" strings into proper nans.\n",
    "df[\"style\"] = df[\"style\"].astype(str)\n",
    "# Discard the 'format' prefix and any non-alphabetic characters, keep any text following the first alphabetic character up to the '} suffix.\n",
    "df[\"style\"] = df[\"style\"].str.extract(r\"{'Format[^a-zA-Z]*([a-zA-Z].*?)'}\")\n",
    "styles = df[\"style\"].value_counts(dropna=False)\n",
    "# There's a few (small) categories that seem unlikely to be \"digital music\". Let's drop them.\n",
    "drop_categories = [\"Paperback\", \"Hardcover\", \"Kindle Edition\", \"USB Memory Stick\", \"Accessory\", \"Health and Beauty\", \"Calendar\",\n",
    "                   \"Unknown Binding\", \"Spiral-bound\", \"Mass Market Paperback\", \"Kitchen\", \"Apparel\", \"Personal Computers\",\n",
    "                   \"Office Product\", \"Grocery\", \"Unbound\", \"Audible Audiobook\", \"Perfect Paperback\", \"Misc. Supplies\", \"Home\"]\n",
    "df = df.loc[~df[\"style\"].isin(drop_categories)]\n",
    "\n",
    "# Don't need to check verified status unique values since we've already seen it's bool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab2a94fc-5d66-4c56-9e92-e9b97038a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert review text to lower case.\n",
    "df[\"reviewText\"] = df[\"reviewText\"].str.lower()\n",
    "# Remove punctuation\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "\n",
    "# Remove stop words.\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['reviewText'] = df['reviewText'].apply(remove_stopwords)\n",
    "\n",
    "# Apply stemming to simplify text further.\n",
    "stemmer = PorterStemmer()\n",
    "df['stemmedText'] = df['reviewText'].apply(lambda x: stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c65e15d6-d76e-4448-97c5-7c7aae7e66c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled DataFrame uploaded to processed_data/processed_df.pickle in bf-review-nlp\n"
     ]
    }
   ],
   "source": [
    "pickled_df = pickle.dumps(df)\n",
    "blob_name = 'processed_data/processed_df.pickle'\n",
    "\n",
    "# Create a blob and upload the pickled DataFrame\n",
    "blob = bucket.blob(blob_name)\n",
    "blob.upload_from_string(pickled_df)\n",
    "\n",
    "print(f'Pickled DataFrame uploaded to {blob_name} in {bucket_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb138b6a-16af-4bda-856d-9403406a27f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
