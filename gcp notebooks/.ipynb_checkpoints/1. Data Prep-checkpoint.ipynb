{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a90d7f3",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "This script takes the raw data, cleans it and preps it for modelling.\n",
    "\n",
    "This script is almost identical to the main script, only with the addition of some extra code to allow us to run this notebook on GCP's Workbench (due to long local run times). In particular, the additional material is for the importing of raw data and saving of processed data to a GCP bucket. We have to rerun this script instead of using the data we prepped locally due to issues with different versions of Pandas being used to create the .pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf280e8-28e2-43de-b592-0988ce8c8d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\benaf\\anaconda3\\envs\\py39\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\benaf\\anaconda3\\envs\\py39\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: click in c:\\users\\benaf\\anaconda3\\envs\\py39\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\benaf\\anaconda3\\envs\\py39\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\benaf\\anaconda3\\envs\\py39\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\benaf\\anaconda3\\envs\\py39\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc11aa78-a007-434c-adc2-52e81c9762ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "from google.cloud import storage\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96a070-9818-4a68-b9e4-6aac7542b583",
   "metadata": {},
   "source": [
    "First we import our raw data from a GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ace6534-ae65-419b-87bc-6749b9748098",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'bf-review-nlp'\n",
    "blob_name = 'raw_data/Digital_Music.json.gz'\n",
    "\n",
    "# Initialize GCS client and get the blob\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(bucket_name)\n",
    "blob = bucket.blob(blob_name)\n",
    "\n",
    "json_gz_bytes = blob.download_as_bytes()\n",
    "json_gz_file = io.BytesIO(json_gz_bytes)\n",
    "\n",
    "with gzip.open(json_gz_file, 'rb') as f:\n",
    "    df = pd.read_json(f, lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cb6371",
   "metadata": {},
   "source": [
    "Then we can undertake data cleaning by dropping unwanted columns, rows with no review text, checking column types/categories and formatting them properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaa6e7a3-1132-4665-8ca1-9ca16af3a332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1584082 entries, 0 to 1584081\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count    Dtype \n",
      "---  ------      --------------    ----- \n",
      " 0   overall     1584082 non-null  int64 \n",
      " 1   verified    1584082 non-null  bool  \n",
      " 2   style       1310814 non-null  object\n",
      " 3   reviewText  1582629 non-null  object\n",
      "dtypes: bool(1), int64(1), object(2)\n",
      "memory usage: 37.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns for this project.\n",
    "df = df.drop([\"reviewTime\", \"reviewerID\", \"asin\", \"reviewerName\", \"summary\", \"unixReviewTime\", \"vote\", \"image\"], axis=1)\n",
    "\n",
    "# Check NAs and column types.\n",
    "df.info()\n",
    "\n",
    "# Drop rows with no review text.\n",
    "df = df.dropna(subset=[\"reviewText\"])\n",
    "\n",
    "# Check all scores are values 1-5. We convert to categorical because the scores aren't continuous.\n",
    "df[\"overall\"].unique()\n",
    "df[\"overall\"] = df['overall'].astype('category')\n",
    "\n",
    "# Check style values.\n",
    "# We clean up the formatting with regex. This also conveniently converts \"nan\" strings into proper nans.\n",
    "df[\"style\"] = df[\"style\"].astype(str)\n",
    "# Discard the 'format' prefix and any non-alphabetic characters, keep any text following the first alphabetic character up to the '} suffix.\n",
    "df[\"style\"] = df[\"style\"].str.extract(r\"{'Format[^a-zA-Z]*([a-zA-Z].*?)'}\")\n",
    "styles = df[\"style\"].value_counts(dropna=False)\n",
    "# There's a few (small) categories that seem unlikely to be \"digital music\". Let's drop them.\n",
    "drop_categories = [\"Paperback\", \"Hardcover\", \"Kindle Edition\", \"USB Memory Stick\", \"Accessory\", \"Health and Beauty\", \"Calendar\",\n",
    "                   \"Unknown Binding\", \"Spiral-bound\", \"Mass Market Paperback\", \"Kitchen\", \"Apparel\", \"Personal Computers\",\n",
    "                   \"Office Product\", \"Grocery\", \"Unbound\", \"Audible Audiobook\", \"Perfect Paperback\", \"Misc. Supplies\", \"Home\"]\n",
    "df = df.loc[~df[\"style\"].isin(drop_categories)]\n",
    "\n",
    "# Don't need to check verified status unique values since we've already seen it's bool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572d8ed",
   "metadata": {},
   "source": [
    "Next we prepare the data for modelling by converting the review text to lower case, stripping out punctuation and stop words, and using a Stemming method to simplify the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab2a94fc-5d66-4c56-9e92-e9b97038a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert review text to lower case.\n",
    "df[\"reviewText\"] = df[\"reviewText\"].str.lower()\n",
    "# Remove punctuation\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "\n",
    "# Remove stop words.\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['reviewText'] = df['reviewText'].apply(remove_stopwords)\n",
    "\n",
    "# Apply stemming to simplify text further.\n",
    "stemmer = PorterStemmer()\n",
    "df['stemmedText'] = df['reviewText'].apply(lambda x: stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e375d94",
   "metadata": {},
   "source": [
    "Now we save the data back to our Bucket so that we only have to run this script once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c65e15d6-d76e-4448-97c5-7c7aae7e66c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled DataFrame uploaded to processed_data/processed_df.pickle in bf-review-nlp\n"
     ]
    }
   ],
   "source": [
    "pickled_df = pickle.dumps(df)\n",
    "blob_name = 'processed_data/processed_df.pickle'\n",
    "\n",
    "# Create a blob and upload the pickled DataFrame\n",
    "blob = bucket.blob(blob_name)\n",
    "blob.upload_from_string(pickled_df)\n",
    "\n",
    "print(f'Pickled DataFrame uploaded to {blob_name} in {bucket_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
