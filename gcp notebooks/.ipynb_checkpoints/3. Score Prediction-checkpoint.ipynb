{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fecfab18",
   "metadata": {},
   "source": [
    "# Score Prediction\n",
    "This script trials a couple of different modelling approaches using Random Forest and Naive Bayes classifiers to try and predict review scores from the review text. \n",
    "\n",
    "This script is almost identical to the main script, only with the addition of some extra code to allow us to run this notebook on GCP's Workbench (due to long local run times). In particular, the additional material is for the importing of raw data and saving of processed data to a GCP Bucket, and some code select only a subset of the data for demonstration purposes due to run times (even on GCP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9d06d40-d232-424b-b2ad-49c0490eee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from google.cloud import storage\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4d1e10",
   "metadata": {},
   "source": [
    "First we import data from our GCS Bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0525aafb-7c1a-44b8-b257-7d85147676a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"bf-review-nlp\"\n",
    "blob_name = \"processed_data/processed_df.pickle\"\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "blob = bucket.blob(blob_name)\n",
    "pickle_data = blob.download_as_bytes()\n",
    "pickle_file = io.BytesIO(pickle_data)\n",
    "df_raw = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d995f0",
   "metadata": {},
   "source": [
    "Even when running on GCP's Workbench, the run times were extremely long using the full \\~1.5m-record dataset. For demonstration purposes, this code takes a stratified sample from the original dataset comprising 5% of the total rows (~79,000 reviews).\n",
    "\n",
    "We use a stratified split because the classes (review scores 1*-5*) are very unbalanced, as shown in our EDA script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ef00f33-3643-4bb8-b802-0675f6af4109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 79122 entries, 256187 to 892717\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   overall      79122 non-null  category\n",
      " 1   verified     79122 non-null  bool    \n",
      " 2   style        65261 non-null  object  \n",
      " 3   reviewText   79122 non-null  object  \n",
      " 4   stemmedText  79122 non-null  object  \n",
      "dtypes: bool(1), category(1), object(3)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Trim dataset for runtime\n",
    "\n",
    "X_raw = df_raw[\"stemmedText\"]\n",
    "y_raw = df_raw[\"overall\"]\n",
    "stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.05, random_state=42)\n",
    "_, idx = next(stratified_split.split(X_raw, y_raw))\n",
    "\n",
    "df = df_raw.iloc[idx]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b80e8c1",
   "metadata": {},
   "source": [
    "This function will be used to save a number of model metrics to .csv files for evaluation after each model has been run so that we can examine them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f35bdb50-83c6-4e6c-8e1c-bd6673d3ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output classification_report() as .csv\n",
    "def csv_report(report, name):\n",
    "    lines = report.split(\"\\n\")\n",
    "    data = [line.split()[1:] for line in lines[2:-5]]\n",
    "    columns = [\"precision\", \"recall\", \"f1-score\", \"support\"]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    output_csv_path = name+\".csv\"\n",
    "    df.to_csv(output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98814044",
   "metadata": {},
   "source": [
    "Here we transform our review text into an n-gram count matrix (considering only unigrams and bigrams). We also limit the matrix to contain the 1000 most common n-grams just for simplicity and run time. A more advanced model could consider a much larger dictionary.\n",
    "\n",
    "Once again we use a stratified train-test split due to the severely unbalanced classes we saw in our EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57236331-696b-4c7f-be8b-df05f937ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(df[\"stemmedText\"])\n",
    "y = df[\"overall\"]\n",
    "\n",
    "stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(stratified_split.split(X, y))\n",
    "X_train = X[train_idx]\n",
    "X_test = X[test_idx]\n",
    "y_train = y.iloc[train_idx]\n",
    "y_test = y.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9b31d9",
   "metadata": {},
   "source": [
    "Now we train a number of classification models, and output some evaluation metrics using the function we defined above. We will try using the dataset as-is, as well as using class weights to improve performance on highly unbalanced classes.\n",
    "\n",
    "In all cases we will use f1-score as our primary metric, again because we want to assess performance on unbalanced classes.\n",
    "\n",
    "The first model we trial is a Random Forest classifier, using a grid search to tune hyperparameters. The GridSearchCV funtion builds in k-fold cross validation for more reliable evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c2f9c2-42dc-4276-b29b-1a0c79eba101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# We could do a much more comprehensive parameter search, but due to computation time, we'll keep it small.\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf_params = {\"max_depth\": [None, 10],\n",
    "             \"min_samples_split\": [10, 20],\n",
    "             \"min_samples_leaf\": [4, 8]}\n",
    "\n",
    "rf_grid = GridSearchCV(rf, rf_params, cv=5, scoring=\"f1_macro\")\n",
    "rf_grid.fit(X_train, y_train)\n",
    "rf_best_params = rf_grid.best_params_\n",
    "\n",
    "best_rf = RandomForestClassifier(**rf_best_params, random_state=42)\n",
    "best_rf.fit(X_train, y_train)\n",
    "rf_pred = best_rf.predict(X_test)\n",
    "rf_report = classification_report(y_test, rf_pred)\n",
    "csv_report(rf_report, \"rf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77ca73",
   "metadata": {},
   "source": [
    "We will re-use the hyperparameters chosen above to train another Random Forest classifier using class weights for comparison. Ideally we would re-tune hyperparameters, but this is simpler for demonstration purposes.\n",
    "\n",
    "An alternative approach could be oversampling, but class weighting is less computationally expensive and run time is a major concern here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c1f3608-3f65-4506-a913-cf6b26d15b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest classifier with class weights.\n",
    "class_weights = compute_class_weight(\"balanced\", classes=y_train.unique(), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "rf_weighted = RandomForestClassifier(**rf_best_params, class_weight=class_weight_dict, random_state=42)\n",
    "rf_weighted.fit(X_train, y_train)\n",
    "rf_wighted_pred = rf_weighted.predict(X_test)\n",
    "rf_weighted_report = classification_report(y_test, rf_wighted_pred)\n",
    "csv_report(rf_weighted_report, \"rf_weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ee1be6",
   "metadata": {},
   "source": [
    "We repeat the above steps using a Multinomial Naive Bayes classifier. First we train a model using a grid search for hyperparameter tuning, then we re-use the best performing parameters to train a second NB classifier using class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "944ca2c5-faa9-40e7-8b12-02af184a3e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb_params = {\"alpha\": [0.1, 1, 10],\n",
    "             \"fit_prior\": [True, False]}\n",
    "\n",
    "nb_grid = GridSearchCV(nb, nb_params, cv=5, scoring=\"f1_macro\")\n",
    "nb_grid.fit(X_train, y_train)\n",
    "nb_best_params = nb_grid.best_params_\n",
    "\n",
    "best_nb = MultinomialNB(**nb_best_params)\n",
    "best_nb.fit(X_train, y_train)\n",
    "nb_pred = best_nb.predict(X_test)\n",
    "nb_report = classification_report(y_test, nb_pred)\n",
    "csv_report(nb_report, \"nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b70f7d94-2b59-4c56-add1-4553b497b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually set class prior probabilities as the class weights\n",
    "nb_weighted = MultinomialNB(**nb_best_params, class_prior=class_weights)\n",
    "nb_weighted.fit(X_train, y_train)\n",
    "nb_weighted_pred = nb_weighted.predict(X_test)\n",
    "nb_weighted_report = classification_report(y_test, nb_weighted_pred)\n",
    "csv_report(nb_weighted_report, \"nb_weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8285cd13",
   "metadata": {},
   "source": [
    "Now we can compare model performance from the output .csvs.\n",
    "\n",
    "Let's look at macro and weighted-average f1-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3618ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_report = pd.read_csv(\"rf.csv\")\n",
    "rf_weighted_report = pd.read_csv(\"rf_weighted.csv\")\n",
    "nb_report = pd.read_csv(\"nb.csv\")\n",
    "nb_weighted_report = pd.read_csv(\"nb_weighted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d44044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_f1(df, name):\n",
    "    df[\"weighted_f1\"] = df[\"f1-score\"] * df['support']\n",
    "    weighted_average_f1 = df[\"weighted_f1\"].sum() / df[\"support\"].sum()\n",
    "    print(name, \": \", weighted_average_f1)\n",
    "    \n",
    "def macro_f1(df, name):\n",
    "    macro_f1 = df[\"f1-score\"].mean()\n",
    "    print(name, \": \", macro_f1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f96676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted averages\n",
    "weighted_f1(rf_report, \"rf\")\n",
    "weighted_f1(rf_weighted_report, \"rf_weighted\")\n",
    "weighted_f1(nb_report, \"nb\")\n",
    "weighted_f1(nb_weighted_report, \"nb_weighted\")\n",
    "\n",
    "# Macro scores\n",
    "macro_f1(rf_report, \"rf\")\n",
    "macro_f1(rf_weighted_report, \"rf_weighted\")\n",
    "macro_f1(nb_report, \"nb\")\n",
    "macro_f1(nb_weighted_report, \"nb_weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada35c2",
   "metadata": {},
   "source": [
    "With the best weighted- and macro-f1 averages, we could say our \"overall\" best performer is the unweighted Naive Bayes classifier.\n",
    "\n",
    "However, this depends on our use for the model. For example, if we wanted to specifically focus on identifying the differences between 4* and 5* reviews (see \"Additional Insights.py\" script!), we might want to look at the weighted Random Forest model which performs better in those specific two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7aae4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
